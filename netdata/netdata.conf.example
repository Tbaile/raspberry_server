[global]
	# run as user = netdata
	# host access prefix = /host
	# glibc malloc arena max for plugins = 1
	# libuv worker threads = 16
	# hostname = netdata.home
	# enable metric correlations = yes
	# metric correlations method = ks2
	# timezone = UTC
	# OOM score = 0
	# process scheduling policy = batch
	# process nice level = 19
	# pthread stack size = 131072

[db]
	# update every = 1
	# mode = dbengine
	# page cache with malloc = no
	# page cache size MB = 32
	# dbengine disk space MB = 256
	# dbengine multihost disk space MB = 256
	# memory deduplication (ksm) = yes
	# dbengine page fetch timeout secs = 3
	# dbengine page fetch retries = 3
	# cleanup obsolete charts after secs = 3600
	# gap when lost iterations above = 1
	# cleanup orphan hosts after secs = 3600
	# delete obsolete charts files = yes
	# delete orphan hosts files = yes
	# dbengine pages per extent = 64
	# enable zero metrics = no

[directories]
	# config = /etc/netdata
	# stock config = /usr/lib/netdata/conf.d
	# log = /var/log/netdata
	# web = /usr/share/netdata/web
	# cache = /var/cache/netdata
	# lib = /var/lib/netdata
	# home = /root
	# lock = /var/lib/netdata/lock
	# plugins = "/usr/libexec/netdata/plugins.d" "/etc/netdata/custom-plugins.d"
	# registry = /var/lib/netdata/registry
	# health config = /etc/netdata/health.d
	# stock health config = /usr/lib/netdata/conf.d/health.d

[logs]
	# debug flags = 0x0000000000000000
	# debug = /var/log/netdata/debug.log
	# error = /var/log/netdata/error.log
	# access = /var/log/netdata/access.log
	# facility = daemon
	# errors flood protection period = 1200
	# errors to trigger flood protection = 200

[environment variables]
	# PATH = /usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/sbin:/usr/sbin:/usr/local/bin:/usr/local/sbin
	# PYTHONPATH = 
	# TZ = :/etc/localtime

[host labels]
	# name = value

[sqlite]
	# auto vacuum = INCREMENTAL
	# synchronous = NORMAL
	# journal mode = WAL
	# temp store = MEMORY
	# journal size limit = 16777216
	# cache size = -2000

[cloud]
	# proxy = env
	# mqtt5 = no
	# aclk implementation = ng
	# query thread count = 2

[ml]
	# enabled = yes
	# maximum num samples to train = 14400
	# minimum num samples to train = 900
	# train every = 3600
	# dbengine anomaly rate every = 30
	# num samples to diff = 1
	# num samples to smooth = 3
	# num samples to lag = 5
	# random sampling ratio = 0.20000
	# maximum number of k-means iterations = 1000
	# dimension anomaly score threshold = 0.99000
	# host anomaly rate threshold = 0.01000
	# minimum window size = 30.00000
	# maximum window size = 600.00000
	# idle window size = 30.00000
	# window minimum anomaly rate = 0.25000
	# anomaly event min dimension rate threshold = 0.05000
	# hosts to skip from training = !*
	# charts to skip from training = netdata.*
	# stream anomaly detection charts = yes

[health]
	# silencers file = /var/lib/netdata/health.silencers.json
	# enabled = yes
	# default repeat warning = never
	# default repeat critical = never
	# in memory max health log entries = 1000
	# script to execute on alarm = /usr/libexec/netdata/plugins.d/alarm-notify.sh
	# enable stock health configuration = yes
	# rotate log every lines = 2000
	# run at least every seconds = 10
	# postpone alarms during hibernation for seconds = 60

[web]
	# default port = 19999
	# ssl key = /etc/netdata/ssl/key.pem
	# ssl certificate = /etc/netdata/ssl/cert.pem
	# tls version = 1.3
	# tls ciphers = none
	# ses max window = 15
	# des max window = 15
	# mode = static-threaded
	# listen backlog = 4096
	# bind to = *
	# disconnect idle clients after seconds = 60
	# timeout for first request = 60
	# accept a streaming request every seconds = 0
	# respect do not track policy = no
	# x-frame-options response header = 
	# allow connections from = localhost *
	# allow connections by dns = heuristic
	# allow dashboard from = localhost *
	# allow dashboard by dns = heuristic
	# allow badges from = *
	# allow badges by dns = heuristic
	# allow streaming from = *
	# allow streaming by dns = heuristic
	# allow netdata.conf from = localhost fd* 10.* 192.168.* 172.16.* 172.17.* 172.18.* 172.19.* 172.20.* 172.21.* 172.22.* 172.23.* 172.24.* 172.25.* 172.26.* 172.27.* 172.28.* 172.29.* 172.30.* 172.31.* UNKNOWN
	# allow netdata.conf by dns = no
	# allow management from = localhost
	# allow management by dns = heuristic
	# enable gzip compression = yes
	# gzip compression strategy = default
	# gzip compression level = 3
	# web server threads = 4
	# web server max sockets = 262144
	# custom dashboard_info.js = 

[registry]
	# enabled = no
	# netdata unique id file = /var/lib/netdata/registry/netdata.public.unique.id
	# registry db file = /var/lib/netdata/registry/registry.db
	# registry log file = /var/lib/netdata/registry/registry-log.db
	# registry save db every new entries = 1000000
	# registry expire idle persons days = 365
	# registry domain = 
	# registry to announce = https://registry.my-netdata.io
	# registry hostname = netdata.home
	# verify browser cookies support = yes
	# enable cookies SameSite and Secure = yes
	# max URL length = 1024
	# max URL name length = 50
	# netdata management api key file = /var/lib/netdata/netdata.api.key
	# allow from = *
	# allow by dns = heuristic

[global statistics]
	# update every = 1

[plugins]
	# timex = yes
	# checks = no
	# idlejitter = yes
	# tc = yes
	# diskspace = yes
	# proc = yes
	# cgroups = yes
	# enable running new plugins = yes
	# check for new plugins every = 60
	# slabinfo = no
	# ioping = yes
	# go.d = yes
	# charts.d = yes
	# apps = yes
	# python.d = yes
	# freeipmi = yes
	# perf = yes
	# fping = yes
	# statsd = yes

[statsd]
	# update every (flushInterval) = 1
	# udp messages to process at once = 10
	# create private charts for metrics matching = *
	# max private charts allowed = 200
	# max private charts hard limit = 1000
	# private charts memory mode = dbengine
	# private charts history = 3600
	# decimal detail = 1000
	# disconnect idle tcp clients after seconds = 600
	# private charts hidden = no
	# histograms and timers percentile (percentThreshold) = 95.00000
	# dictionaries max unique dimensions = 200
	# add dimension for number of events received = no
	# gaps on gauges (deleteGauges) = no
	# gaps on counters (deleteCounters) = no
	# gaps on meters (deleteMeters) = no
	# gaps on sets (deleteSets) = no
	# gaps on histograms (deleteHistograms) = no
	# gaps on timers (deleteTimers) = no
	# gaps on dictionaries (deleteDictionaries) = no
	# statsd server max TCP sockets = 262144
	# listen backlog = 4096
	# default port = 8125
	# bind to = udp:localhost tcp:localhost

[plugin:idlejitter]
	# loop time in ms = 20

[plugin:timex]
	# update every = 10
	# clock synchronization state = yes
	# time offset = yes

[plugin:ioping]
	# update every = 1
	# command options = 

[plugin:go.d]
	# update every = 1
	# command options = 

[plugin:charts.d]
	# update every = 1
	# command options = 

[plugin:apps]
	# update every = 1
	# command options = 

[plugin:python.d]
	# update every = 1
	# command options = 

[plugin:freeipmi]
	# update every = 1
	# command options = 

[plugin:perf]
	# update every = 1
	# command options = 

[plugin:fping]
	# update every = 1
	# command options = 

[plugin:proc]
	# /proc/net/dev = yes
	# /proc/pagetypeinfo = no
	# /proc/stat = yes
	# /proc/uptime = yes
	# /proc/loadavg = yes
	# /proc/sys/kernel/random/entropy_avail = yes
	# /proc/pressure = yes
	# /proc/interrupts = yes
	# /proc/softirqs = yes
	# /proc/vmstat = yes
	# /proc/meminfo = yes
	# /sys/kernel/mm/ksm = yes
	# /sys/block/zram = yes
	# /sys/devices/system/edac/mc = yes
	# /sys/devices/system/node = yes
	# /proc/net/wireless = yes
	# /proc/net/sockstat = yes
	# /proc/net/sockstat6 = yes
	# /proc/net/netstat = yes
	# /proc/net/snmp = yes
	# /proc/net/snmp6 = yes
	# /proc/net/sctp/snmp = yes
	# /proc/net/softnet_stat = yes
	# /proc/net/ip_vs/stats = yes
	# /sys/class/infiniband = yes
	# /proc/net/stat/conntrack = yes
	# /proc/net/stat/synproxy = yes
	# /proc/diskstats = yes
	# /proc/mdstat = yes
	# /proc/net/rpc/nfsd = yes
	# /proc/net/rpc/nfs = yes
	# /proc/spl/kstat/zfs/arcstats = yes
	# /proc/spl/kstat/zfs/pool/state = yes
	# /sys/fs/btrfs = yes
	# ipc = yes
	# /sys/class/power_supply = yes

[plugin:proc:diskspace]
	# remove charts of unmounted disks = yes
	# update every = 1
	# check for new mount points every = 15
	# exclude space metrics on paths = /proc/* /sys/* /var/run/user/* /run/user/* /snap/* /var/lib/docker/*
	# exclude space metrics on filesystems = *gvfs *gluster* *s3fs *ipfs *davfs2 *httpfs *sshfs *gdfs *moosefs fusectl autofs
	# space usage for all disks = auto
	# inodes usage for all disks = auto

[plugin:tc]
	# script to run to get tc values = /usr/libexec/netdata/plugins.d/tc-qos-helper.sh

[plugin:cgroups]
	# update every = 1
	# check for new cgroups every = 10
	# use unified cgroups = auto
	# containers priority = 40000
	# enable cpuacct stat (total CPU) = auto
	# enable cpuacct usage (per core CPU) = auto
	# enable cpuacct cpu throttling = yes
	# enable cpuacct cpu shares = no
	# enable memory = auto
	# enable detailed memory = auto
	# enable memory limits fail count = auto
	# enable swap memory = auto
	# enable blkio bandwidth = auto
	# enable blkio operations = auto
	# enable blkio throttle bandwidth = auto
	# enable blkio throttle operations = auto
	# enable blkio queued operations = auto
	# enable blkio merged operations = auto
	# enable cpu pressure = auto
	# enable io some pressure = auto
	# enable io full pressure = auto
	# enable memory some pressure = auto
	# enable memory full pressure = auto
	# recheck zero blkio every iterations = 10
	# recheck zero memory failcnt every iterations = 10
	# recheck zero detailed memory every iterations = 10
	# enable systemd services = yes
	# enable systemd services detailed memory = no
	# report used memory = yes
	# path to /sys/fs/cgroup/cpuacct = /host/sys/fs/cgroup/cpu,cpuacct
	# path to /sys/fs/cgroup/cpuset = /host/sys/fs/cgroup/cpuset
	# path to /sys/fs/cgroup/blkio = /host/sys/fs/cgroup/blkio
	# path to /sys/fs/cgroup/memory = /host/sys/fs/cgroup/memory
	# path to /sys/fs/cgroup/devices = /host/sys/fs/cgroup/devices
	# max cgroups to allow = 1000
	# max cgroups depth to monitor = 0
	# enable by default cgroups matching =  !*/init.scope  !/system.slice/run-*.scope  *.scope  /machine.slice/*.service  /kubepods/pod*/*  /kubepods/*/pod*/*  !/kubepods*  !*/vcpu*  !*/emulator  !*.mount  !*.partition  !*.service  !*.socket  !*.slice  !*.swap  !*.user  !/  !/docker  !*/libvirt  !/lxc  !/lxc/*/*  !/lxc.monitor*  !/lxc.pivot  !/lxc.payload  !/machine  !/qemu  !/system  !/systemd  !/user  * 
	# enable by default cgroups names matching =  * 
	# search for cgroups in subpaths matching =  !*/init.scope  !*-qemu  !*.libvirt-qemu  !/init.scope  !/system  !/systemd  !/user  !/user.slice  !/lxc/*/*  !/lxc.monitor  !/lxc.payload/*/*  !/lxc.payload.*  * 
	# script to get cgroup names = /usr/libexec/netdata/plugins.d/cgroup-name.sh
	# script to get cgroup network interfaces = /usr/libexec/netdata/plugins.d/cgroup-network
	# run script to rename cgroups matching =  !/  !*.mount  !*.socket  !*.partition  /machine.slice/*.service  !*.service  !*.slice  !*.swap  !*.user  !init.scope  !*.scope/vcpu*  !*.scope/emulator  *.scope  *docker*  *lxc*  *qemu*  /kubepods/pod*/*  /kubepods/*/pod*/*  !/kubepods*  *.libvirt-qemu  * 
	# cgroups to match as systemd services =  !/system.slice/*/*.service  /system.slice/*.service 
	# meminfo filename to monitor = /host/proc/meminfo
